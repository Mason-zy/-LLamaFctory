# vLLM å­¦ä¹ ä»»åŠ¡æ¸…å•

**æ¨¡å—å‘¨æœŸ**ï¼šDay 3-4ï¼ˆ2 å¤©ï¼‰
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­ï¼ˆä¸­ç­‰ï¼‰
**å‰ç½®è¦æ±‚**ï¼šå·²å®Œæˆ Day 1 å•å¡æ¨ç†å†’çƒŸ

---

## ğŸ“ æ‰§è¡Œæ—¥å¿—ï¼ˆå®æ—¶æ›´æ–°ï¼‰

### 2026-01-09 | Day 3 æ‰§è¡Œå¼€å§‹

#### âœ… æ­¥éª¤ 1ï¼šæ£€æŸ¥ GPU èµ„æº
```bash
nvidia-smi
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: ç¡®è®¤è‡³å°‘æœ‰ä¸€å¼ ç©ºé—² GPUï¼ˆæ˜¾å­˜ > 16GBï¼‰
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 2ï¼šå®‰è£… vLLM
```bash
conda activate videofen
pip install vllm
python -c "import vllm; print(vllm.__version__)"
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: æ˜¾ç¤º vllm ç‰ˆæœ¬å·
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 3ï¼šå®‰è£…ç›‘æ§å·¥å…·
```bash
pip install nvitop
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 4ï¼šå•å¡éƒ¨ç½² 7B æ¨¡å‹
```bash
export HF_ENDPOINT=https://hf-mirror.com
CUDA_VISIBLE_DEVICES=0 vllm serve Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 2048
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: çœ‹åˆ° "Uvicorn running on http://0.0.0.0:8000"
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 5ï¼šGPU ç›‘æ§ï¼ˆæ–°ç»ˆç«¯ï¼‰
```bash
nvitop
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: GPU 0 æ˜¾å­˜å ç”¨çº¦ 7-8GBï¼ŒGPU åˆ©ç”¨ç‡ > 80%
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 6ï¼šAPI æµ‹è¯•ï¼ˆç¬¬ä¸‰ä¸ªç»ˆç«¯ï¼‰
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
    "temperature": 0.7
  }'
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: è¿”å› JSON æ ¼å¼çš„æ¨¡å‹å›å¤
**å®é™…ç»“æœ**: å¾…è®°å½•

---

#### âœ… æ­¥éª¤ 7ï¼šæŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
```bash
curl http://localhost:8000/metrics
```
**çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
**é¢„æœŸ**: æ˜¾ç¤º vLLM æ€§èƒ½æŒ‡æ ‡
**å®é™…ç»“æœ**: å¾…è®°å½•

---

### ğŸ“Š Day 3 éªŒæ”¶è¿›åº¦
- [ ] vLLM æœåŠ¡æˆåŠŸå¯åŠ¨
- [ ] API è¯·æ±‚è¿”å›æ­£å¸¸å“åº”
- [ ] nvitop æ˜¾ç¤ºæ˜¾å­˜åˆ©ç”¨ç‡ > 90%
- [ ] GPU åˆ©ç”¨ç‡ > 80%

---

## ğŸ“‹ å­¦ä¹ ç›®æ ‡

- [ ] ç†è§£ vLLM çš„æ ¸å¿ƒä»·å€¼ï¼ˆé«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼‰
- [ ] æŒæ¡ PagedAttention åŸç†ï¼ˆæ˜¾å­˜ç®¡ç†é©å‘½ï¼‰
- [ ] æŒæ¡ Continuous Batching åŸç†ï¼ˆååé‡ä¼˜åŒ–ï¼‰
- [ ] åŒå¡å¼ é‡å¹¶è¡Œéƒ¨ç½² 14B æ¨¡å‹
- [ ] OpenAI å…¼å®¹ API æµ‹è¯•ä¸æ€§èƒ½å¯¹æ¯”

---

## ğŸ“… Day 3ï¼šåŸºç¡€ç†è®ºä¸å•å¡éƒ¨ç½²

### ä»»åŠ¡æ¸…å•

#### ä¸Šåˆï¼šç†è®ºå­¦ä¹ ï¼ˆ2-3 å°æ—¶ï¼‰
- [ ] é˜…è¯» `modules/03_vllm/readme.md` ç¬¬ 1-3 ç« 
  - [ ] ç†è§£ vLLM åœ¨å·¥å…·é“¾ä¸­çš„ä½ç½®ï¼ˆæ¨ç† vs è®­ç»ƒï¼‰
  - [ ] ç†è§£ PagedAttention æœºåˆ¶ï¼ˆåˆ†é¡µå¼æ˜¾å­˜ç®¡ç†ï¼‰
  - [ ] ç†è§£ Continuous Batchingï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
  - [ ] ç†è§£æ ¸å¿ƒæŒ‡æ ‡ï¼ˆTTFTã€TPOTã€Throughputï¼‰

- [ ] å®Œæˆç†è®ºè‡ªæµ‹é¢˜
  ```
  Q1: vLLM ä¸ºä»€ä¹ˆèƒ½æ¯” HuggingFace æ¨ç†å¿« 3-10 å€ï¼Ÿ
  Q2: PagedAttention å’Œæ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜æœ‰ä»€ä¹ˆç±»æ¯”å…³ç³»ï¼Ÿ
  Q3: Continuous Batching è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
  ```

#### ä¸‹åˆï¼šç¯å¢ƒå‡†å¤‡ä¸å®‰è£…ï¼ˆ1-2 å°æ—¶ï¼‰
- [ ] æ£€æŸ¥ GPU èµ„æº
  ```bash
  nvidia-smi
  # ç¡®è®¤è‡³å°‘æœ‰ä¸€å¼ ç©ºé—² GPUï¼ˆå»ºè®®æ˜¾å­˜ > 16GBï¼‰
  ```

- [ ] å®‰è£… vLLM
  ```bash
  conda activate videofen
  pip install vllm

  # éªŒè¯å®‰è£…
  python -c "import vllm; print(vllm.__version__)"
  ```

- [ ] ï¼ˆå¯é€‰ï¼‰å®‰è£…ç›‘æ§å·¥å…·
  ```bash
  pip install nvitop gpustat
  ```

#### æ™šä¸Šï¼šå•å¡éƒ¨ç½²å†’çƒŸï¼ˆ2-3 å°æ—¶ï¼‰
- [ ] ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰
  ```bash
  # ä½¿ç”¨ HF é•œåƒä¸‹è½½
  export HF_ENDPOINT=https://hf-mirror.com
  huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
    --local-dir /path/to/models/Qwen2.5-7B-Instruct \
    --local-dir-use-symlinks False
  ```

- [ ] å•å¡éƒ¨ç½² 7B æ¨¡å‹
  ```bash
  CUDA_VISIBLE_DEVICES=0 vllm serve Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 2048
  ```

- [ ] OpenAI API æµ‹è¯•
  ```bash
  curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "Qwen/Qwen2.5-7B-Instruct",
      "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
      "temperature": 0.7
    }'
  ```

- [ ] GPU ç›‘æ§éªŒè¯
  ```bash
  # æ‰“å¼€æ–°ç»ˆç«¯è¿è¡Œ
  nvitop
  # è§‚å¯Ÿï¼š
  # - æ˜¾å­˜åˆ©ç”¨ç‡æ˜¯å¦ > 90%
  # - GPU åˆ©ç”¨ç‡æ˜¯å¦ > 80%
  ```

**Day 3 éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] vLLM æœåŠ¡æˆåŠŸå¯åŠ¨
- [ ] API è¯·æ±‚è¿”å›æ­£å¸¸å“åº”
- [ ] æ˜¾å­˜åˆ©ç”¨ç‡ > 90%
- [ ] èƒ½ç”¨ nvitop ç›‘æ§ GPU çŠ¶æ€

---

## ğŸ“… Day 4ï¼šåŒå¡éƒ¨ç½²ä¸æ€§èƒ½æµ‹è¯•

### ä»»åŠ¡æ¸…å•

#### ä¸Šåˆï¼šåŒå¡å¼ é‡å¹¶è¡Œï¼ˆ3-4 å°æ—¶ï¼‰
- [ ] ç†è§£å¼ é‡å¹¶è¡ŒåŸç†
  - [ ] é˜…è¯»ç†è®ºï¼šå¼ é‡å¹¶è¡Œ vs æ•°æ®å¹¶è¡Œ
  - [ ] ç†è§£ä¸ºä»€ä¹ˆè¦ç”¨å¼ é‡å¹¶è¡Œï¼ˆå•å¡æ˜¾å­˜ä¸è¶³ï¼‰

- [ ] ä¸‹è½½ 14B æ¨¡å‹ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰
  ```bash
  export HF_ENDPOINT=https://hf-mirror.com
  huggingface-cli download Qwen/Qwen2.5-14B-Instruct \
    --local-dir /path/to/models/Qwen2.5-14B-Instruct \
    --local-dir-use-symlinks False
  ```

- [ ] åŒå¡éƒ¨ç½² 14B æ¨¡å‹
  ```bash
  CUDA_VISIBLE_DEVICES=0,1 vllm serve Qwen/Qwen2.5-14B-Instruct \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 2048 \
    --port 8000
  ```

- [ ] éªŒè¯åŒå¡è´Ÿè½½å‡è¡¡
  ```bash
  nvitop
  # è§‚å¯Ÿä¸¤å¼ å¡çš„æ˜¾å­˜å ç”¨æ˜¯å¦å‡è¡¡ï¼ˆçº¦ 10-12GB/å¡ï¼‰
  ```

- [ ] API åŠŸèƒ½æµ‹è¯•
  ```bash
  curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "Qwen/Qwen2.5-14B-Instruct",
      "messages": [{"role": "user", "content": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}],
      "max_tokens": 512
    }'
  ```

#### ä¸‹åˆï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆ3-4 å°æ—¶ï¼‰
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ç¼–å†™
  ```python
  # benchmark_vllm.py
  import time
  import requests
  import json

  def benchmark_vllm(prompt, num_runs=10):
    url = "http://localhost:8000/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
      "model": "Qwen/Qwen2.5-14B-Instruct",
      "messages": [{"role": "user", "content": prompt}],
      "max_tokens": 256
    }

    latencies = []
    for _ in range(num_runs):
      start = time.time()
      response = requests.post(url, headers=headers, json=data)
      end = time.time()
      latencies.append((end - start) * 1000)  # ms

    return {
      "avg_latency_ms": sum(latencies) / len(latencies),
      "min_latency_ms": min(latencies),
      "max_latency_ms": max(latencies)
    }

  if __name__ == "__main__":
    result = benchmark_vllm("è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ")
    print(json.dumps(result, indent=2))
  ```

- [ ] è¿è¡ŒåŸºå‡†æµ‹è¯•
  ```bash
  python benchmark_vllm.py
  ```

- [ ] å¯¹æ¯” vLLM vs HuggingFace
  | æŒ‡æ ‡ | vLLM | HuggingFace | æå‡å€æ•° |
  |------|------|-------------|----------|
  | æ˜¾å­˜åˆ©ç”¨ç‡ | ? | ? | ? |
  | å¹³å‡å»¶è¿Ÿ | ? | ? | ? |
  | ååé‡ | ? | ? | ? |

#### æ™šä¸Šï¼šç›‘æ§ä¸æ—¥å¿—ï¼ˆ1-2 å°æ—¶ï¼‰
- [ ] æŸ¥çœ‹ vLLM å†…ç½® Metrics
  ```bash
  curl http://localhost:8000/metrics
  ```

- [ ] å…³é”®æŒ‡æ ‡è§£è¯»
  - `vllm:num_requests_running`: è¿è¡Œä¸­çš„è¯·æ±‚æ•°
  - `vllm:num_requests_waiting`: æ’é˜Ÿä¸­çš„è¯·æ±‚æ•°
  - `vllm:gpu_cache_usage_perc`: KV Cache æ˜¾å­˜ä½¿ç”¨ç‡
  - `vllm:time_to_first_token_ms`: TTFT
  - `vllm:time_per_output_token_ms`: TPOT

- [ ] ï¼ˆå¯é€‰ï¼‰Prometheus + Grafana ç›‘æ§
  - [ ] éƒ¨ç½² Prometheus
  - [ ] é…ç½® Grafana Dashboard
  - [ ] å®æ—¶ç›‘æ§ vLLM æ€§èƒ½æŒ‡æ ‡

**Day 4 éªŒæ”¶æ ‡å‡†**ï¼š
- [ ] åŒå¡ 14B æ¨¡å‹æˆåŠŸéƒ¨ç½²
- [ ] ä¸¤å¼ å¡æ˜¾å­˜å ç”¨å‡è¡¡ï¼ˆè¯¯å·® < 10%ï¼‰
- [ ] å®Œæˆæ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] èƒ½è§£è¯»å…³é”®æ€§èƒ½æŒ‡æ ‡
- [ ] ï¼ˆå¯é€‰ï¼‰æ­å»ºç›‘æ§ Dashboard

---

## ğŸ¯ æ¨¡å—éªŒæ”¶æ ‡å‡†

### ç†è®ºéªŒæ”¶
- [ ] èƒ½ç”¨è‡ªå·±çš„è¯è§£é‡Š PagedAttention åŸç†
- [ ] èƒ½ç”¨è‡ªå·±çš„è¯è§£é‡Š Continuous Batching ä¼˜åŠ¿
- [ ] èƒ½è¯´æ˜å¼ é‡å¹¶è¡Œä¸æ•°æ®å¹¶è¡Œçš„åŒºåˆ«

### å®æ“éªŒæ”¶
- [ ] å•å¡ 7B æ¨¡å‹éƒ¨ç½²æˆåŠŸï¼ˆAPI å¯ç”¨ï¼‰
- [ ] åŒå¡ 14B æ¨¡å‹éƒ¨ç½²æˆåŠŸï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
- [ ] å®Œæˆæ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆvLLM vs HFï¼‰
- [ ] èƒ½ä½¿ç”¨ nvitop/gpustat ç›‘æ§ GPU

### è¾“å‡ºç‰©
- [ ] æ€§èƒ½å¯¹æ¯”æŠ¥å‘Šï¼ˆè¡¨æ ¼å½¢å¼ï¼‰
- [ ] éƒ¨ç½²å‘½ä»¤ç¬”è®°ï¼ˆå«å‚æ•°è¯´æ˜ï¼‰
- [ ] ï¼ˆå¯é€‰ï¼‰ç›‘æ§ Dashboard æˆªå›¾

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [PagedAttention è®ºæ–‡](https://arxiv.org/abs/2309.06180)

### æ¨èé˜…è¯»
- `modules/03_vllm/readme.md`ï¼ˆå®Œæ•´ç†è®ºæŒ‡å—ï¼‰
- [Continuous Batching æŠ€æœ¯è§£æ](https://luyuhuang.github.io/2023/08/23/continuous-batching.html)

### å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥
```bash
# å¯åŠ¨ vLLM æœåŠ¡
vllm serve <model_path> [options]

# æ ¸å¿ƒå‚æ•°
--tensor-parallel-size <n>      # å¼ é‡å¹¶è¡Œ GPU æ•°
--gpu-memory-utilization <0.9>  # GPU æ˜¾å­˜åˆ©ç”¨ç‡
--max-model-len <2048>          # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
--host 0.0.0.0                   # ç›‘å¬åœ°å€
--port 8000                      # ç›‘å¬ç«¯å£

# æŸ¥çœ‹æŒ‡æ ‡
curl http://localhost:8000/metrics
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: vLLM å¯åŠ¨æŠ¥æ˜¾å­˜ä¸è¶³ï¼Ÿ
**A**: é™ä½ `gpu-memory-utilization` æˆ– `max-model-len`
```bash
vllm serve model --gpu-memory-utilization 0.7 --max-model-len 1024
```

### Q2: åŒå¡éƒ¨ç½²æ—¶æ˜¾å­˜ä¸å‡è¡¡ï¼Ÿ
**A**: æ£€æŸ¥ `CUDA_VISIBLE_DEVICES` è®¾ç½®ï¼Œç¡®ä¿ä¸¤å¼ å¡éƒ½å¯è§
```bash
# æŸ¥çœ‹å¯è§ GPU
python -c "import torch; print(torch.cuda.device_count())"
```

### Q3: API è¯·æ±‚è¶…æ—¶ï¼Ÿ
**A**: å¢åŠ  `max-model-len` æˆ–é™ä½è¯·æ±‚å¹¶å‘æ•°

---

## ğŸ”„ ä¸åç»­æ¨¡å—çš„è¡”æ¥

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- âœ… ç”Ÿäº§çº§æ¨ç†å¼•æ“çš„ä½¿ç”¨
- âœ… å¤šå¡å¼ é‡å¹¶è¡Œéƒ¨ç½²
- âœ… æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜

**ä¸‹ä¸€æ¨¡å—**ï¼šDay 5-6 Accelerate åˆ†å¸ƒå¼è®­ç»ƒ
- å­¦ä¹ å¦‚ä½•ç»Ÿä¸€ç®¡ç†å•å¡/å¤šå¡è®­ç»ƒ
- ä¸º DeepSpeed æ˜¾å­˜ä¼˜åŒ–æ‰“åŸºç¡€
