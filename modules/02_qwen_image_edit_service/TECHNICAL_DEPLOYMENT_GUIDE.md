# Qwen-Image-Edit-2511 å…¨æ ˆéƒ¨ç½²ä¸æ¨ç†æŒ‡å—
**æ–‡æ¡£ç‰ˆæœ¬**ï¼š2.0.0  
**å‘å¸ƒæ—¥æœŸ**ï¼š2025-12-26  
**é€‚ç”¨ç¯å¢ƒ**ï¼šLinux (CentOS/Ubuntu), CUDA 12+, PyTorch 2.3+  
---
## ğŸ“‹ æ–‡æ¡£ç›®å½•
1. [æŠ€æœ¯æ¦‚è¿°](#1-æŠ€æœ¯æ¦‚è¿°)  
2. [æ¨¡å‹åŸç†ä¸æ ¼å¼](#2-æ¨¡å‹åŸç†ä¸æ ¼å¼)  
3. [æ¨ç†å·¥å…·å¯¹æ¯”](#3-æ¨ç†å·¥å…·å¯¹æ¯”)  
4. [éƒ¨ç½²æ¶æ„è®¾è®¡](#4-éƒ¨ç½²æ¶æ„è®¾è®¡)  
5. [ç¯å¢ƒæ„å»ºä¸ä¾èµ–ç®¡ç†](#5-ç¯å¢ƒæ„å»ºä¸ä¾èµ–ç®¡ç†)  
6. [æ¨¡å‹èµ„äº§æœ¬åœ°åŒ–](#6-æ¨¡å‹èµ„äº§æœ¬åœ°åŒ–)  
7. [æ ¸å¿ƒä»£ç å®ç°](#7-æ ¸å¿ƒä»£ç å®ç°)  
8. [Web æœåŠ¡å°è£…](#8-web-æœåŠ¡å°è£…)  
9. [èµ„æºè°ƒåº¦ä¸ä¼˜åŒ–](#9-èµ„æºè°ƒåº¦ä¸ä¼˜åŒ–)  
10. [ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆ](#10-ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆ)  
11. [æ•…éšœæ’æŸ¥æ‰‹å†Œ](#11-æ•…éšœæ’æŸ¥æ‰‹å†Œ)  
12. [é™„å½•ï¼šå®Œæ•´ä»£ç ](#12-é™„å½•å®Œæ•´ä»£ç )  
---

æ•ˆæœå›¾ï¼š
![!\[åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\](https://i-blog.csdnimg.cn/direct/5b0b709a](https://i-blog.csdnimg.cn/direct/d52bced4163948a887cd85eb542ba19a.png)




## 1. æŠ€æœ¯æ¦‚è¿°
æœ¬æŒ‡å—æ—¨åœ¨é˜è¿°å¦‚ä½•åœ¨ç§æœ‰åŒ–åŸºç¡€è®¾æ–½ä¸­ï¼ŒåŸºäº `Qwen/Qwen-Image-Edit-2511` æ¨¡å‹æ„å»ºé«˜å¯ç”¨ã€å¯æ‰©å±•çš„å›¾åƒç¼–è¾‘æ¨ç†æœåŠ¡ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨ä¸šç•Œæ ‡å‡†çš„ `diffusers` æ¨ç†æ¡†æ¶ï¼Œç»“åˆä¸­å›½å¤§é™†ç½‘ç»œç¯å¢ƒä¸‹çš„å·¥ç¨‹åŒ–é€‚é…ï¼Œè§£å†³äº†æ¨¡å‹èµ„äº§ç®¡ç†ã€å¼‚æ„è®¡ç®—èµ„æºè°ƒåº¦ï¼ˆGPU/CPU é™çº§ï¼‰ã€æ˜¾å­˜ä¼˜åŒ–åŠæœåŠ¡åŒ–å°è£…ç­‰æ ¸å¿ƒé—®é¢˜ã€‚
**æ ¸å¿ƒä»·å€¼**ï¼š
*   **å¼€ç®±å³ç”¨**ï¼šæä¾›æ ‡å‡†åŒ–çš„ç¯å¢ƒæ„å»ºä¸æ¨¡å‹åŠ è½½æµç¨‹ã€‚
*   **èµ„æºå¼¹æ€§**ï¼šæ”¯æŒåœ¨ GPU æ˜¾å­˜å—é™æˆ–è¢«å ç”¨åœºæ™¯ä¸‹ï¼Œè‡ªåŠ¨/æ‰‹åŠ¨é™çº§è‡³ CPU æ¨ç†ï¼Œç¡®ä¿æœåŠ¡å¯ç”¨æ€§ã€‚
*   **å·¥ç¨‹è§„èŒƒ**ï¼šéµå¾ªç”Ÿäº§çº§ç›®å½•ç»“æ„ã€æ—¥å¿—è§„èŒƒä¸é…ç½®ç®¡ç†ã€‚
---
## 2. æ¨¡å‹åŸç†ä¸æ ¼å¼
### 2.1 æ¨¡å‹æ¶æ„
`Qwen-Image-Edit` åŸºäºæŒ‡ä»¤é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹ï¼ˆInstruction-based Diffusion Modelï¼‰æ¶æ„ã€‚å…¶æ¨ç†è¿‡ç¨‹é€šè¿‡ `QwenImageEditPlusPipeline` å®ç°ï¼Œè¯¥ Pipeline ç¼–æ’äº†ä»¥ä¸‹å…³é”®ç»„ä»¶ï¼š
*   **Text Encoder (Qwen2-VL)**: è´Ÿè´£ç†è§£å¤šæ¨¡æ€æŒ‡ä»¤ï¼ˆPromptï¼‰ï¼Œå°†è‡ªç„¶è¯­è¨€ç¼–è¾‘è¯·æ±‚è½¬æ¢ä¸ºè¯­ä¹‰åµŒå…¥ï¼ˆEmbeddingsï¼‰ã€‚
*   **VAE (Variational Autoencoder)**: è´Ÿè´£å›¾åƒçš„æ½œåœ¨ç©ºé—´ï¼ˆLatent Spaceï¼‰ç¼–è§£ç ï¼Œå°†é«˜ç»´åƒç´ æ•°æ®å‹ç¼©ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤ºï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚
*   **UNet / DiT**: æ ¸å¿ƒå»å™ªç½‘ç»œï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ ¹æ®æ–‡æœ¬æ¡ä»¶ä¸è¾“å…¥å›¾åƒç‰¹å¾ï¼Œé€æ­¥å»é™¤å™ªå£°ä»¥é‡æ„ç›®æ ‡å›¾åƒã€‚
*   **Scheduler**: å™ªå£°è°ƒåº¦å™¨ï¼Œæ§åˆ¶é‡‡æ ·æ­¥æ•°ï¼ˆStepsï¼‰ä¸å»å™ªè½¨è¿¹ï¼Œå¹³è¡¡ç”Ÿæˆè´¨é‡ä¸æ¨ç†å»¶è¿Ÿã€‚
### 2.2 æ¨¡å‹æ ¼å¼å¯¹æ¯”
| æ ¼å¼ | å¼€å‘è€… | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | Qwen-Image-Edit æ”¯æŒ |
|------|--------|------|----------|---------------------|
| **Safetensors** | Hugging Face | âœ… å®‰å…¨ã€å¿«é€Ÿã€è·¨æ¡†æ¶ | Diffusers æ¨ç† | âœ… å®˜æ–¹æ¨è |
| **GGUF** | llama.cpp | âœ… é‡åŒ–ã€å•æ–‡ä»¶ | Ollama/è½»é‡çº§ | âŒ éœ€è½¬æ¢ |
| **PyTorch .bin** | PyTorch | âœ… ä¼ ç»Ÿæ ¼å¼ | æ—§ç‰ˆç³»ç»Ÿ | âŒ ä¸æ¨è |
| **ONNX** | Microsoft | âœ… è·¨å¹³å° | éƒ¨ç½²ä¼˜åŒ– | âŒ éœ€è½¬æ¢ |
**å…³é”®ç‚¹**ï¼šQwen-Image-Edit-2511 ä½¿ç”¨ Safetensors æ ¼å¼ï¼Œè¿™æ˜¯ Hugging Face ç”Ÿæ€çš„æ ‡å‡†æ ¼å¼ã€‚
---
## 3. æ¨ç†å·¥å…·å¯¹æ¯”
| å·¥å…· | æ ¸å¿ƒä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | Qwen-Image-Edit æ”¯æŒ |
|------|----------|----------|---------------------|
| **vLLM** | âš¡ é«˜ååé‡ã€OpenAI å…¼å®¹ | LLM æ–‡æœ¬ç”Ÿæˆ | âŒ ä¸ç›´æ¥æ”¯æŒï¼ˆéœ€é€‚é…ï¼‰ |
| **Diffusers** | âœ… å®˜æ–¹æ”¯æŒã€å¤šæ¨¡æ€ | å›¾åƒç”Ÿæˆ/ç¼–è¾‘ | âœ… å®Œç¾æ”¯æŒ |
| **FastAPI** | âœ… ç”Ÿäº§çº§ API æœåŠ¡ | ä¼ä¸šé›†æˆ | âœ… éœ€è‡ªè¡Œå°è£… |
| **Gradio** | âœ… å¿«é€Ÿ Web UI | æ¼”ç¤º/æµ‹è¯• | âœ… å®Œç¾æ”¯æŒ |
| **A1111/ComfyUI** | âœ… å¯è§†åŒ–å·¥ä½œæµ | ä¸ªäººä½¿ç”¨ | âš ï¸ éœ€è‡ªå®šä¹‰èŠ‚ç‚¹ |
**æ¨èæ–¹æ¡ˆ**ï¼š
- **æ¨ç†å±‚**ï¼šDiffusersï¼ˆå®˜æ–¹æ”¯æŒï¼‰
- **æœåŠ¡å±‚**ï¼šFastAPIï¼ˆç”Ÿäº§ APIï¼‰ + Gradioï¼ˆWeb UIï¼‰
---
## 4. éƒ¨ç½²æ¶æ„è®¾è®¡
### 4.1 å•æœºéƒ¨ç½²æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®¢æˆ·ç«¯ (æµè§ˆå™¨/ä¸šåŠ¡ç³»ç»Ÿ)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP/REST
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API ç½‘å…³ / Gradio UI                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¨ç†æœåŠ¡ (Python + Diffusers)                         â”‚
â”‚  â”œâ”€â”€ æ¨¡å‹åŠ è½½ä¸ç¼“å­˜                                    â”‚
â”‚  â”œâ”€â”€ èµ„æºç®¡ç†ï¼ˆæ˜¾å­˜/CPUï¼‰                              â”‚
â”‚  â””â”€â”€ æ¨ç†æ‰§è¡Œ                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### 4.2 å¤šæœº/å®¹å™¨åŒ–æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è´Ÿè½½å‡è¡¡å™¨ (Nginx/Kong)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¨ç†æœåŠ¡é›†ç¾¤ (Kubernetes)                             â”‚
â”‚  â”œâ”€â”€ æœåŠ¡ 1: GPU èŠ‚ç‚¹ (diffusers)                      â”‚
â”‚  â”œâ”€â”€ æœåŠ¡ 2: CPU é™çº§èŠ‚ç‚¹ (diffusers)                  â”‚
â”‚  â””â”€â”€ æœåŠ¡ 3: ç›‘æ§/æ—¥å¿— (Prometheus/ELK)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---
## 5. ç¯å¢ƒæ„å»ºä¸ä¾èµ–ç®¡ç†
### 5.1 ç¡¬ä»¶ä¸ç³»ç»Ÿè¦æ±‚
| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® | è¯´æ˜ |
|------|----------|----------|------|
| **GPU** | 24GB VRAM (RTX 3090) | 48GB+ VRAM (A800/A100) | æ”¯æŒ BF16/FP16 |
| **CPU** | 8 vCPU | 32 vCPU+ | CPU é™çº§æ¨¡å¼ |
| **RAM** | 32GB | 64GB+ | æ¨¡å‹åŠ è½½ä¸ Offload |
| **Disk** | 50GB SSD | 100GB+ SSD | æ¨¡å‹æƒé‡ + ç¼“å­˜ |
### 5.2 è½¯ä»¶ä¾èµ–æ ˆ
```bash
# åˆ›å»º Conda ç¯å¢ƒ
conda create -n qwen_edit python=3.10 -y
conda activate qwen_edit
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install git+https://github.com/huggingface/diffusers
pip install accelerate transformers protobuf sentencepiece
```
### 5.3 ç½‘ç»œé€‚é…ç­–ç•¥
```bash
# é…ç½®ç¯å¢ƒå˜é‡
export HF_ENDPOINT=https://hf-mirror.com
export HF_HOME=/path/to/your/cache
export QWEN_EDIT_2511_DIR=/path/to/your/models/Qwen-Image-Edit-2511
export HF_HUB_OFFLINE=1  # ç”Ÿäº§ç¯å¢ƒå¼ºåˆ¶ç¦»çº¿
```
---
## 6. æ¨¡å‹èµ„äº§æœ¬åœ°åŒ–
```python
from huggingface_hub import snapshot_download
import os
snapshot_download(
    repo_id="Qwen/Qwen-Image-Edit-2511",
    local_dir=os.environ.get("QWEN_EDIT_2511_DIR"),
    resume_download=True,
    local_dir_use_symlinks=False,
    ignore_patterns=["*.msgpack", "*.h5"]
)
```
---
## 7. æ ¸å¿ƒä»£ç å®ç°
### 7.1 æ¨¡å‹åŠ è½½ä¸ä¼˜åŒ–
```python
import torch
from diffusers import QwenImageEditPlusPipeline
def load_pipeline(model_dir, use_cpu_offload=False):
    # ç²¾åº¦é€‰æ‹©
    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    
    # åŠ è½½ Pipeline
    pipe = QwenImageEditPlusPipeline.from_pretrained(
        model_dir,
        torch_dtype=dtype,
        variant="bf16" if dtype == torch.bfloat16 else None
    )
    
    # æ˜¾å­˜ä¼˜åŒ–
    if use_cpu_offload:
        pipe.enable_model_cpu_offload()
    else:
        pipe.to("cuda")
    
    # VAE åˆ†å—è§£ç 
    pipe.enable_vae_tiling()
    
    return pipe
```
### 7.2 æ¨ç†æ‰§è¡Œ
```python
def run_inference(pipe, image_path, prompt):
    input_image = Image.open(image_path).convert("RGB")
    generator = torch.Generator(device=pipe.device).manual_seed(42)
    
    output = pipe(
        prompt=prompt,
        image=input_image,
        num_inference_steps=30,
        guidance_scale=1.0,
        true_cfg_scale=4.0,
        generator=generator
    )
    
    return output.images[0]
```
---
## 8. Web æœåŠ¡å°è£…
### 8.1 FastAPI æœåŠ¡
```python
from fastapi import FastAPI, File, UploadFile
from pydantic import BaseModel
app = FastAPI()
class EditRequest(BaseModel):
    prompt: str
    seed: int = 42
    steps: int = 30
    true_cfg_scale: float = 4.0
    guidance_scale: float = 1.0
@app.post("/edit")
async def edit_image(request: EditRequest, file: UploadFile = File(...)):
    # æ¨¡å‹åŠ è½½ä¸æ¨ç†é€»è¾‘
    pass
```
### 8.2 Gradio äº¤äº’ç•Œé¢
```python
import gradio as gr
def main():
    with gr.Blocks(title="Qwen-Image-Edit-2511") as demo:
        gr.Markdown("# å›¾åƒç¼–è¾‘æœåŠ¡")
        
        with gr.Row():
            with gr.Column():
                image_in = gr.Image(type="pil", label="è¾“å…¥å›¾ç‰‡")
                prompt = gr.Textbox(lines=3, label="ç¼–è¾‘éœ€æ±‚")
                run = gr.Button("ç”Ÿæˆ")
            
            with gr.Column():
                image_out = gr.Image(type="pil", label="è¾“å‡ºç»“æœ")
        
        run.click(
            fn=edit_image,
            inputs=[image_in, prompt],
            outputs=[image_out]
        )
    
    demo.launch(server_name="0.0.0.0", server_port=7860)
if __name__ == "__main__":
    main()
```
---
## 9. èµ„æºè°ƒåº¦ä¸ä¼˜åŒ–
### 9.1 æ˜¾å­˜ç®¡ç†ç­–ç•¥
```python
# å¤š GPU è‡ªåŠ¨åˆ†ç‰‡
if gpu_count >= 2:
    max_memory = {}
    for i in range(gpu_count):
        total_gib = int(torch.cuda.get_device_properties(i).total_memory / (1024**3))
        max_gib = max(4, total_gib - 6)  # é¢„ç•™ 6GB æ˜¾å­˜
        max_memory[i] = f"{max_gib}GiB"
```
### 9.2 CPU é™çº§ä¼˜åŒ–
```python
def _maybe_limit_resources():
    # é™åˆ¶ CPU çº¿ç¨‹æ•°
    torch.set_num_threads(max(1, (os.cpu_count() or 1) // 2))
    
    # é™ä½è¿›ç¨‹ä¼˜å…ˆçº§
    try:
        os.nice(5)
    except Exception:
        pass
```
---
## 10. ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆ
### 10.1 Docker å®¹å™¨åŒ–
```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
ENV HF_ENDPOINT=https://hf-mirror.com
ENV QWEN_EDIT_2511_DIR=/app/models
CMD ["python", "gradio_app.py"]
```
### 10.2 Kubernetes éƒ¨ç½²
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-edit
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: qwen-edit
        image: your-registry/qwen-edit:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        env:
        - name: QWEN_EDIT_FORCE_CPU
          value: "0"
```
---
## 11. æ•…éšœæ’æŸ¥æ‰‹å†Œ
| é”™è¯¯ç°è±¡ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|
| **CUDA out of memory** | æ˜¾å­˜ä¸è¶³ | 1. å¼€å¯ Model Offload<br>2. é™ä½åˆ†è¾¨ç‡<br>3. å¢åŠ  Headroom |
| **ç½‘ç»œé”™è¯¯** | HF è®¿é—®é—®é¢˜ | 1. æ£€æŸ¥ HF_ENDPOINT<br>2. ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½<br>3. è®¾ç½® HF_HUB_OFFLINE=1 |
| **æ¨ç†å¡ä½** | CPU è´Ÿè½½é«˜ | 1. é™åˆ¶çº¿ç¨‹æ•°<br>2. é™ä½é‡‡æ ·æ­¥æ•°<br>3. æ£€æŸ¥è¿›ç¨‹ä¼˜å…ˆçº§ |
| **å›¾ç‰‡å…¨é»‘** | VAE é—®é¢˜ | 1. å¼€å¯ VAE Tiling<br>2. åˆ‡æ¢ FP32 æµ‹è¯•<br>3. æ£€æŸ¥è¾“å…¥æ ¼å¼ |
---
## 12. é™„å½•ï¼šå®Œæ•´ä»£ç 
### 12.1 ç¯å¢ƒé…ç½®è„šæœ¬
```bash
#!/bin/bash
# setup_env.sh
# åˆ›å»º Conda ç¯å¢ƒ
conda create -n qwen_edit python=3.10 -y
conda activate qwen_edit
# å®‰è£…ä¾èµ–
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install git+https://github.com/huggingface/diffusers
pip install accelerate transformers protobuf sentencepiece
# é…ç½®ç¯å¢ƒå˜é‡
echo "export HF_ENDPOINT=https://hf-mirror.com" >> ~/.bashrc
echo "export HF_HOME=/home/user/cache" >> ~/.bashrc
echo "export QWEN_EDIT_2511_DIR=/home/user/models/Qwen-Image-Edit-2511" >> ~/.bashrc
echo "export HF_HUB_OFFLINE=1" >> ~/.bashrc
source ~/.bashrc
```
### 12.2 æ¨¡å‹ä¸‹è½½è„šæœ¬
```python
# download_model.py
from huggingface_hub import snapshot_download
import os
def download_qwen_edit():
    model_dir = os.environ.get("QWEN_EDIT_2511_DIR")
    if not model_dir:
        raise RuntimeError("QWEN_EDIT_2511_DIR not set")
    
    snapshot_download(
        repo_id="Qwen/Qwen-Image-Edit-2511",
        local_dir=model_dir,
        resume_download=True,
        local_dir_use_symlinks=False
    )
if __name__ == "__main__":
    download_qwen_edit()
```
### 12.3 å®Œæ•´ Gradio åº”ç”¨
```python
import os
from typing import Optional

import gradio as gr
import torch
from PIL import Image


def _get_model_dir() -> str:
    model_dir = os.environ.get("QWEN_EDIT_2511_DIR")
    if not model_dir:
        raise RuntimeError(
            "Missing env var QWEN_EDIT_2511_DIR. "
            "Set it to your local model directory, e.g. /home/zzy/weitiao/models/Qwen-Image-Edit-2511"
        )
    return model_dir


def _force_cpu() -> bool:
    return os.environ.get("QWEN_EDIT_FORCE_CPU", "0") == "1"


def _maybe_limit_resources() -> None:
    """Best-effort resource limits to avoid overloading the host.

    - Caps torch CPU threads (intra/inter-op)
    - Optionally lowers process priority (nice)
    """

    # Default: use about half of the machine cores.
    default_threads = max(1, (os.cpu_count() or 1) // 2)
    max_threads = int(os.environ.get("QWEN_EDIT_MAX_CPU_THREADS", str(default_threads)))
    max_threads = max(1, max_threads)

    # Lower priority so background services stay responsive.
    try:
        nice_delta = int(os.environ.get("QWEN_EDIT_NICE", "5"))
        if nice_delta != 0:
            os.nice(nice_delta)
    except Exception:
        pass

    try:
        torch.set_num_threads(max_threads)
        torch.set_num_interop_threads(min(4, max_threads))
    except Exception:
        pass


_PIPE = None


def _format_gib(num_bytes: int) -> str:
    return f"{num_bytes / (1024 ** 3):.2f} GiB"


def _assert_vram_headroom() -> None:
    """Fail fast if visible GPUs are already heavily occupied.

    This prevents confusing OOMs during `from_pretrained()` that are actually caused by other jobs
    (e.g. vLLM workers) using most of the VRAM.
    """

    if not torch.cuda.is_available():
        return

    min_free_gib = float(os.environ.get("QWEN_EDIT_MIN_FREE_GIB", "6"))
    min_free_bytes = int(min_free_gib * (1024 ** 3))

    bad = []
    for i in range(torch.cuda.device_count()):
        free_b, total_b = torch.cuda.mem_get_info(i)
        if free_b < min_free_bytes:
            bad.append((i, free_b, total_b))

    if bad:
        details = ", ".join(
            [f"cuda:{i} free={_format_gib(free_b)}/{_format_gib(total_b)}" for i, free_b, total_b in bad]
        )
        raise RuntimeError(
            "Not enough free VRAM on visible GPUs. "
            f"Need >= {min_free_gib:.0f} GiB free per GPU, but got: {details}. "
            "Please stop other GPU jobs first (check with `nvidia-smi -i 4,5,6,7`)."
        )


def _get_pipe():
    global _PIPE
    if _PIPE is not None:
        return _PIPE

    from diffusers import QwenImageEditPlusPipeline

    model_dir = _get_model_dir()

    if _force_cpu():
        pipe = QwenImageEditPlusPipeline.from_pretrained(model_dir, torch_dtype=torch.float32)
        pipe.to("cpu")
        pipe.set_progress_bar_config(disable=None)
        for method_name, args in (
            ("enable_attention_slicing", ("max",)),
            ("enable_vae_slicing", ()),
            ("enable_vae_tiling", ()),
        ):
            fn = getattr(pipe, method_name, None)
            if callable(fn):
                try:
                    fn(*args)
                except Exception:
                    pass
        _PIPE = pipe
        return _PIPE

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is not available (set QWEN_EDIT_FORCE_CPU=1 to run on CPU)")

    _assert_vram_headroom()

    # If multiple GPUs are visible, shard the pipeline across GPUs to reduce per-GPU VRAM.
    # This is the simplest way to make big diffusion models fit when a single 24GB card is not enough.
    gpu_count = torch.cuda.device_count()
    if gpu_count >= 2:
        # Leave some headroom on each GPU for activations/temporary buffers to reduce OOM risk.
        # NOTE: When CUDA_VISIBLE_DEVICES=4,5,6,7, the visible GPU indices are 0..3.
        headroom_gib = int(os.environ.get("QWEN_EDIT_GPU_HEADROOM_GIB", "6"))
        gpu0_extra_headroom_gib = int(os.environ.get("QWEN_EDIT_GPU0_EXTRA_HEADROOM_GIB", "4"))
        max_memory = {}
        for i in range(gpu_count):
            total_gib = int(torch.cuda.get_device_properties(i).total_memory / (1024**3))
            effective_headroom = headroom_gib + (gpu0_extra_headroom_gib if i == 0 else 0)
            max_gib = max(4, total_gib - effective_headroom)
            max_memory[i] = f"{max_gib}GiB"
        # Allow offload if needed.
        max_memory["cpu"] = os.environ.get("QWEN_EDIT_CPU_MAX_MEMORY", "120GiB")

        offload_folder = os.environ.get(
            "QWEN_EDIT_OFFLOAD_FOLDER", "/home/zzy/weitiao/cache/offload/qwen_image_edit_2511"
        )
        os.makedirs(offload_folder, exist_ok=True)

        torch_dtype = torch.bfloat16 if os.environ.get("QWEN_EDIT_DTYPE", "bf16") == "bf16" else torch.float16

        pipe = QwenImageEditPlusPipeline.from_pretrained(
            model_dir,
            torch_dtype=torch_dtype,
            device_map="balanced",
            max_memory=max_memory,
            low_cpu_mem_usage=True,
            offload_state_dict=True,
            offload_folder=offload_folder,
        )
    else:
        torch_dtype = torch.bfloat16 if os.environ.get("QWEN_EDIT_DTYPE", "bf16") == "bf16" else torch.float16
        pipe = QwenImageEditPlusPipeline.from_pretrained(model_dir, torch_dtype=torch_dtype)
        pipe.to("cuda")
    pipe.set_progress_bar_config(disable=None)

    # Reduce peak memory during VAE + attention.
    for method_name, args in (
        ("enable_attention_slicing", ("max",)),
        ("enable_vae_slicing", ()),
        ("enable_vae_tiling", ()),
    ):
        fn = getattr(pipe, method_name, None)
        if callable(fn):
            try:
                fn(*args)
            except Exception:
                pass

    _PIPE = pipe
    return _PIPE


def _maybe_resize(image: Image.Image, max_side: int) -> Image.Image:
    if max_side <= 0:
        return image

    w, h = image.size
    if max(w, h) <= max_side:
        return image

    scale = max_side / float(max(w, h))
    new_w = max(1, int(round(w * scale)))
    new_h = max(1, int(round(h * scale)))
    return image.resize((new_w, new_h), Image.LANCZOS)


@torch.inference_mode()
def edit_image(
    image: Optional[Image.Image],
    prompt: str,
    seed: int,
    num_inference_steps: int,
    true_cfg_scale: float,
    guidance_scale: float,
    max_side: int,
):
    if image is None:
        raise gr.Error("è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡")
    if not prompt or not prompt.strip():
        raise gr.Error("è¯·å…ˆè¾“å…¥ç¼–è¾‘éœ€æ±‚ï¼ˆpromptï¼‰")

    if (not _force_cpu()) and (not torch.cuda.is_available()):
        raise gr.Error("CUDA ä¸å¯ç”¨ï¼šè¯·æ£€æŸ¥ NVIDIA é©±åŠ¨ä¸ torch CUDA ç¯å¢ƒï¼ˆæˆ–è®¾ç½® QWEN_EDIT_FORCE_CPU=1 ç”¨ CPU å†’çƒŸï¼‰")

    pipe = _get_pipe()

    image = image.convert("RGB")
    image = _maybe_resize(image, max_side=max_side)

    generator_device = "cpu" if _force_cpu() else "cuda:0"
    if seed < 0:
        generator = torch.Generator(device=generator_device).seed()
    else:
        generator = torch.Generator(device=generator_device).manual_seed(int(seed))

    inputs = {
        "image": [image],
        "prompt": prompt.strip(),
        "generator": generator,
        "true_cfg_scale": float(true_cfg_scale),
        "negative_prompt": " ",
        "num_inference_steps": int(num_inference_steps),
        "guidance_scale": float(guidance_scale),
        "num_images_per_prompt": 1,
    }

    out = pipe(**inputs)
    return out.images[0]


def main():
    title = "Qwen-Image-Edit-2511 æœ¬åœ°äº¤äº’å¼ Demo"

    _maybe_limit_resources()

    # Eager-load the pipeline at startup so OOMs show immediately (instead of after clicking).
    if os.environ.get("QWEN_EDIT_EAGER_LOAD", "1") == "1":
        _get_pipe()

    with gr.Blocks(title=title) as demo:
        gr.Markdown(
            """
# Qwen-Image-Edit-2511ï¼ˆæœ¬åœ°äº¤äº’å¼ï¼‰

- ä¸Šä¼ å›¾ç‰‡ â†’ è¾“å…¥ç¼–è¾‘éœ€æ±‚ â†’ ç‚¹å‡»ç”Ÿæˆ â†’ è¿”å›ç»“æœå›¾
- å»ºè®®å…ˆç¡®ä¿å·²å®Œæˆæ¨¡å‹ä¸‹è½½ï¼Œå¹¶è®¾ç½®ï¼š`QWEN_EDIT_2511_DIR` æŒ‡å‘æœ¬åœ°æ¨¡å‹ç›®å½•
""".strip()
        )

        with gr.Row():
            with gr.Column(scale=1):
                image_in = gr.Image(type="pil", label="è¾“å…¥å›¾ç‰‡")
                prompt = gr.Textbox(lines=3, label="ç¼–è¾‘éœ€æ±‚ï¼ˆPromptï¼‰")

                with gr.Row():
                    seed = gr.Number(value=0, precision=0, label="Seedï¼ˆ-1 éšæœºï¼‰")
                    steps = gr.Slider(minimum=10, maximum=80, step=1, value=40, label="Steps")

                with gr.Row():
                    true_cfg = gr.Slider(minimum=1.0, maximum=8.0, step=0.1, value=4.0, label="true_cfg_scale")
                    guidance = gr.Slider(minimum=0.5, maximum=3.0, step=0.1, value=1.0, label="guidance_scale")

                max_side = gr.Slider(
                    minimum=0,
                    maximum=2048,
                    step=64,
                    value=768,
                    label="æœ€å¤§è¾¹é•¿ï¼ˆ>0 æ—¶è‡ªåŠ¨ç¼©æ”¾ï¼Œé¿å… OOMï¼‰",
                )

                run = gr.Button("ç”Ÿæˆ", variant="primary")

            with gr.Column(scale=1):
                image_out = gr.Image(type="pil", label="è¾“å‡ºç»“æœ")

        run.click(
            fn=edit_image,
            inputs=[image_in, prompt, seed, steps, true_cfg, guidance, max_side],
            outputs=[image_out],
        )

    demo.queue(max_size=20, default_concurrency_limit=int(os.environ.get("QWEN_EDIT_MAX_CONCURRENCY", "1")))
    demo.launch(server_name="0.0.0.0", server_port=int(os.environ.get("PORT", "7860")))


if __name__ == "__main__":
    main()

```
---
## ğŸ“š æ–‡æ¡£ä½¿ç”¨è¯´æ˜
1. **ç¯å¢ƒå‡†å¤‡**ï¼šè¿è¡Œ `setup_env.sh` è„šæœ¬
2. **æ¨¡å‹ä¸‹è½½**ï¼šè¿è¡Œ `download_model.py`
3. **å¯åŠ¨æœåŠ¡**ï¼šè¿è¡Œ `gradio_app.py`
4. **è®¿é—®ç•Œé¢**ï¼šæµè§ˆå™¨è®¿é—® `http://your-server:7860`
**é…ç½®è°ƒæ•´**ï¼š
- ä¿®æ”¹ç¯å¢ƒå˜é‡è°ƒæ•´èµ„æºé™åˆ¶
- è°ƒæ•´ `max_side` æ§åˆ¶å›¾åƒåˆ†è¾¨ç‡
- è°ƒæ•´ `true_cfg_scale` æ§åˆ¶ç¼–è¾‘å¼ºåº¦
---
