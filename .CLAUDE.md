**[CONTEXT HANDOFF - 请阅读以下项目背景与环境状态]**

## 目的
快速熟悉本机环境与资源约束，按“先冒烟后放大”路径完成大模型推理/微调。

## 环境快照
- 硬件：2× Intel Xeon Gold 6530（64 核，AMX/AVX512）、1 TB DDR5、8× RTX 4090 (24 GB)、CentOS Stream 9
- Conda 环境：videofen
- 关键库：PyTorch 2.3.1 + CUDA 12.1
- 工作目录：/home/zzy/weitiao

## 资源使用约束
- GPU：优先使用当前空闲的卡（运行前先看 `nvidia-smi`）。本机最近空闲示例：单卡 `CUDA_VISIBLE_DEVICES=6`；两卡 `CUDA_VISIBLE_DEVICES=6,7`。
  - 如命令里还有 `--gpus`：它是“可见 GPU 的索引”（从 0 开始），不是物理卡号。
- CPU：仅用约一半核心，避免占满整机

## 最小可行流程（五步闭环，先小后大）
1) 单卡文本推理冒烟（约半天）
```bash
CUDA_VISIBLE_DEVICES=6 llamafactory-cli chat \
  --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
  --template qwen
```
出现 “>> User:” 并正常回复即通过。

2) 单卡多模态冒烟（同上，改模型与模板）
```bash
CUDA_VISIBLE_DEVICES=6 llamafactory-cli chat \
  --model_name_or_path Qwen/Qwen2-VL-7B-Instruct \
  --template qwen2_vl
```
图片描述正常即通过。

3) 多卡推理搬运（张量并行；按“当前空闲卡数”选择进程数）
```bash
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num_processes 2 src/train.py \
  --stage inference \
  --model_name_or_path Qwen/Qwen2.5-7B-Instruct \
  --template qwen \
  --infer_backend huggingface \
  --gpus 0,1
```
预期：每卡显存约 4 GB，吞吐约单卡 2.3×。

4) 官方文本 LoRA 示例复现（ZeRO-3，勿改超参，仅改数据路径）
```bash
CUDA_VISIBLE_DEVICES=6,7 deepspeed --num_gpus 2 src/train.py \
  --deepspeed examples/deepspeed/ds_z3_bf16.json \
  --stage sft \
  --config_file examples/train_lora/llama3_lora_sft.yaml
```
训练后用同一 config，把 adapter_name_or_path 指向产物再推理，验证对话可复现。

5) 多模态同流程（模型换 Qwen2-VL-7B，数据换图文混合，其他不变）
数据示例：COCO 2017 抽样 1k 图 + 自写 caption，按 LLaMA-Factory 统一格式。

6) 可选上强度（32B/70B）
- 32B：Qwen2.5-32B，BF16-LoRA，4 卡峰值约 22 GB/卡
- 70B：Llama-3.1-70B，NF4-QLoRA，4 卡峰值约 20 GB/卡
记录：显存峰值、训练速度 (tokens/s)、MT-bench 得分。

## 验证与节奏
- 每步先跑最小规模冒烟，确认无 OOM/加载错误再放大
- 任何新模型或新数据，先单卡文本→单卡多模态→多卡并行→再上强度

## 提醒
- 保持日志，关注显存与吞吐；异常优先减小 batch/序列长
- 若需更多 GPU，请先确认资源占用再申请